**1. What are the most common pitfalls when creating a custom transpiler like Dingo, and how can they be avoided?**

*   **Complexity Creep**: Starting with simple textual replacements but then needing full AST manipulation for more advanced features. This often leads to an inefficient hybrid approach. Avoid by clearly defining the boundary between text-based preprocessing and AST transformation from the outset, and committing to using a robust AST library (like `go/ast` and `astutil`) for anything beyond trivial syntax changes.
*   **Maintaining Go Idiomaticity**: Generating Go code that looks alien or performs poorly because the transpiler prioritizes Dingo's syntax over Go's conventions. This makes the generated code hard to debug, maintain, and integrate with existing Go tools. Avoid by having a strong understanding of idiomatic Go and designing transformation rules to produce clean, readable Go output, even if it means more complex internal transpiler logic.
*   **Lack of Tooling Integration (LSP)**: Without robust IDE support, developers won't adopt a new language or meta-language. Manual source code navigation and debugging are unacceptable for modern development. Avoid by prioritizing LSP integration (like `dingo-lsp` wrapping `gopls`) early in the development cycle. Source maps are critical here.
*   **Debugging Transpiled Code**: When an error occurs in the generated Go code, it needs to be easily mappable back to the original Dingo source. Without this, debugging becomes a nightmare. Avoid by implementing high-quality source map generation from day one, allowing IDEs to highlight errors in the original `.dingo` files.
*   **Performance Overhead**: If the transpilation process itself is slow, or if the generated Go code has significant runtime overhead, adoption will suffer. Avoid by profiling the transpiler's performance and optimizing transformation algorithms. For runtime, rigorously ensure the generated Go is as efficient as hand-written code.
*   **Staying Up-to-Date with Go**: Go's evolution (new features, syntax changes) means the transpiler must constantly adapt. Falling behind can break compatibility. Avoid by closely tracking Go releases and ensuring the transpiler's AST parsing and code generation logic is flexible enough to accommodate changes or quickly adapt to them.
*   **Error Reporting**: Providing cryptic error messages from the Go compiler instead of clear, Dingo-specific diagnostics. This frustrates users. Avoid by building a sophisticated error reporting mechanism that uses source maps to translate Go compiler errors back to meaningful Dingo errors, including suggested fixes.

**2. Given Dingo's two-stage transpilation (Preprocessor + AST), what are the critical quality gates or checks you'd implement at each stage to ensure correctness and maintainability?**

**Stage 1: Preprocessor (Text-based transformation)**

*   **Input Validation**: Before any preprocessing, validate `.dingo` file syntax against basic Dingo-specific rules (e.g., proper keyword usage, balanced delimiters for custom constructs). Fail early if the input is malformed.
*   **Idempotence Check**: Ensure that running the preprocessor multiple times on the same input yields the same output. This prevents subtle bugs from accumulating. Output deterministic `go fmt` and `goimports` formatted output from this stage. This greatly reduces AST diff noise later.
*   **Go Lexical/Syntactic Validity Check**: After preprocessing, the output should be valid Go *lexically* and *syntactically*, even if not semantically correct yet. Use `go/parser` in `ParseComments` mode to parse the preprocessed output (without any AST modifications) to catch syntax errors introduced by the preprocessor.
*   **Golden Tests for Preprocessor**: Extensive golden tests specifically for the preprocessor. Each `.dingo` input should have an expected preprocessed `.go.golden` output that reflects only the text-based transformations.
*   **Source Map Generation**: The preprocessor *must* generate initial source maps mapping the preprocessed Go back to the original Dingo. This is crucial for later debugging and error reporting.

**Stage 2: AST Processing (go/parser + Plugin Pipeline)**

*   **AST Parsing Success**: The first critical check is successful parsing of the preprocessed Go output into a `go/ast` tree. Any errors here indicate a failure in either the preprocessor or a fundamental incompatibility with `go/parser`.
*   **Semantic Analysis (go/types)**: After initial AST transformations, run `go/types` analysis against the generated AST. This will catch type errors, undefined references, and other semantic issues that a simple parser cannot. This is a powerful quality gate for the generated Go's correctness.
*   **Plugin Contract Enforcement**: For each plugin in the pipeline, define clear pre-conditions and post-conditions. Unit tests for each plugin should ensure it correctly transforms only its intended AST nodes without unintended side effects.
*   **AST Diffing (Golden Tests)**: Advanced golden tests that compare the *final* generated `.go` file's AST against an expected `.go.golden` file's AST. Tools like `go/ast/diff` or a custom AST comparison utility can highlight subtle differences that affect correctness or idiomaticity.
*   **Compilability and Executability**: The ultimate quality gate. The fully generated `.go` file must compile successfully without warnings and pass runtime tests. This often involves creating a temporary `main` package and running `go build` and `go run` or `go test`.
*   **Source Map Consistency**: Verify that the source maps generated/updated during AST processing still accurately map back to the original `.dingo` and account for all transformations. This can be done by intentionally introducing errors in the `.dingo` and checking if the reported line numbers from the Go compiler (translated via sourcemap) are correct.
*   **Performance Metrics**: Monitor the time taken for parsing and AST transformation. Large codebases can quickly become unwieldy if this stage is slow.

By implementing these checks, Dingo can ensure a high degree of correctness, maintainability, and provide a superior developer experience with accurate error reporting and reliable code generation. The multi-stage golden testing approach (preprocessor-specific and end-to-end AST comparison) is particularly vital.