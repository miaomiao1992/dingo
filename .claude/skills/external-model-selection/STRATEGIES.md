# Consultation Strategies - Deep Dive

**Purpose**: Detailed guide to each consultation strategy with concrete examples, execution patterns, and success criteria.

---

## Strategy 1: Fast Parallel Diagnosis

**Models**: `minimax/minimax-m2` + `x-ai/grok-code-fast-1`
**Time**: ~4 minutes total
**Success Rate**: 95%+
**Cost**: $$ (moderate)
**Use**: 90% of everyday tasks

### When to Use

✅ **Perfect for**:
- Bug investigations (most common)
- Quick root cause diagnosis
- Production issues requiring fast turnaround
- Error messages needing explanation
- Code behavior mysteries

❌ **Not ideal for**:
- Major architectural decisions (use Strategy 2)
- Highly ambiguous problems (use Strategy 3)
- When you need long-term refactoring plans

### Why This Works

**MiniMax M2** excels at finding the simplest root cause quickly:
- Focuses on obvious bugs first
- Avoids overcomplicating solutions
- Fast execution (3 minutes)
- 91/100 accuracy rating

**Grok Code Fast** provides validation and debugging:
- Step-by-step execution traces
- Identifies edge cases
- Practical test strategies
- 83/100 accuracy rating

**Together**: Quick diagnosis + validation = high confidence solution in 4 minutes

### Execution Pattern

```bash
# Step 1: Create session
SESSION=$(date +%Y%m%d-%H%M%S)
mkdir -p ai-docs/sessions/$SESSION/{input,output}

# Step 2: Write investigation prompt
cat > ai-docs/sessions/$SESSION/input/investigation-prompt.md <<'EOF'
# Bug Investigation

## Problem
[Clear description of the issue]

## Context
[Relevant file paths, error messages, expected vs actual behavior]

## Goal
Identify root cause and propose fix
EOF
```

**Step 3: Launch both models in parallel** (single message):

```python
# Task 1: MiniMax M2
Task tool → golang-architect (PROXY MODE):

You are operating in PROXY MODE using minimax/minimax-m2.

INPUT FILES:
- ai-docs/sessions/$SESSION/input/investigation-prompt.md

YOUR TASK:
1. Read investigation prompt
2. Invoke minimax/minimax-m2 via claudish
3. Write full analysis to output file

Bash(
    command='cat ai-docs/sessions/$SESSION/input/investigation-prompt.md | claudish --model minimax/minimax-m2 > ai-docs/sessions/$SESSION/output/minimax-m2-analysis.md 2>&1',
    timeout=600000,  # 10 minutes
    description='MiniMax M2 consultation'
)

RETURN (MAX 3 lines):
MiniMax M2 analysis complete
Root cause: [one-line]
Full analysis: ai-docs/sessions/$SESSION/output/minimax-m2-analysis.md
```

```python
# Task 2: Grok Code Fast (same message!)
Task tool → golang-architect (PROXY MODE):

[Same structure for x-ai/grok-code-fast-1]
```

### Expected Output

**MiniMax M2 returns**:
```
MiniMax M2 analysis complete
Root cause: qPos calculation uses Index instead of LastIndex
Full analysis: ai-docs/sessions/$SESSION/output/minimax-m2-analysis.md
```

**Grok Code Fast returns**:
```
Grok Code Fast analysis complete
Root cause: generated_column mismatch due to prefix length calculation
Full analysis: ai-docs/sessions/$SESSION/output/grok-analysis.md
```

### Consolidation

After receiving both summaries:

1. **Check for consensus**:
   - Both point to similar root cause? High confidence!
   - Different theories? Read full analyses to understand why

2. **Choose best solution**:
   - MiniMax M2 usually has simplest fix
   - Grok provides validation approach

3. **Implement**:
   - Use MiniMax M2's fix
   - Apply Grok's test strategy

### Success Example

**LSP Source Mapping Bug** (Session 20251118-223538):

- **MiniMax M2**: "Bug is qPos calculation, use LastIndex"
- **Grok**: "generated_column doesn't match actual position"
- **Consensus**: Both identified column calculation issue
- **Result**: ✅ Simple one-line fix implemented and tested
- **Time**: 4 minutes from start to solution

---

## Strategy 2: Comprehensive Analysis

**Models**: `minimax/minimax-m2` + `openai/gpt-5.1-codex` + `x-ai/grok-code-fast-1`
**Time**: ~5 minutes total
**Success Rate**: 99%+
**Cost**: $$$ (high)
**Use**: Critical bugs, architectural decisions

### When to Use

✅ **Perfect for**:
- Critical production bugs
- Architectural decisions with long-term impact
- High-stakes changes (refactoring core systems)
- When you need absolute certainty
- Design reviews before implementation

❌ **Not ideal for**:
- Simple bugs (use Strategy 1)
- Exploratory investigations (use Strategy 3)
- Cost-sensitive projects (use Strategy 4)

### Why This Works

**Three perspectives ensure comprehensive coverage**:

1. **MiniMax M2**: Fast, simple solution (quick win)
2. **GPT-5.1 Codex**: Architectural vision (long-term plan)
3. **Grok Code Fast**: Validation strategy (confidence boost)

**Result**: Immediate fix + future improvements + testing approach = complete solution

### Execution Pattern

Same as Strategy 1, but launch 3 agents instead of 2.

**Step 3: Launch all three models in parallel** (single message with 3 Task calls)

### Expected Output

**MiniMax M2**:
```
Quick fix identified: [one-line]
```

**GPT-5.1 Codex**:
```
Architectural recommendation: [future improvements]
```

**Grok Code Fast**:
```
Validation strategy: [testing approach]
```

### Consolidation

1. **Implement quick fix** (from MiniMax M2)
2. **Plan long-term improvements** (from GPT-5.1)
3. **Execute test strategy** (from Grok)

### Use Case Example

**Major Refactoring Decision**:

**Question**: Should we migrate from regex preprocessor to tree-sitter parser?

**MiniMax M2**: "Regex is working fine, only migrate if you have specific limitations"
**GPT-5.1 Codex**: "Long-term benefits of tree-sitter: better error recovery, incremental parsing, but 3-month migration timeline"
**Grok**: "Test both approaches in parallel branch, measure performance difference, validate edge cases"

**Decision**: Stay with regex for now (MiniMax M2), plan tree-sitter for v2.0 (GPT-5.1), create benchmark suite (Grok)

---

## Strategy 3: Deep Exploration

**Models**: `minimax/minimax-m2` + `google/gemini-2.5-flash` + `x-ai/grok-code-fast-1`
**Time**: ~6 minutes total
**Success Rate**: 90%+
**Cost**: $$ (moderate)
**Use**: Ambiguous, multi-faceted problems

### When to Use

✅ **Perfect for**:
- Ambiguous bugs with unclear root cause
- Multi-faceted problems (could be many causes)
- When initial investigation is inconclusive
- Complex system interactions
- Research and exploration

❌ **Not ideal for**:
- Clear, straightforward bugs (use Strategy 1)
- Time-critical issues (use Strategy 1)
- Budget-constrained projects (use Strategy 4)

### Why This Works

**Gemini 2.5 Flash** excels at exhaustive analysis:
- Explores multiple hypotheses
- Identifies edge cases
- Thorough investigation
- Low cost ($ instead of $$)

**Combined with**:
- **MiniMax M2**: Quick hypothesis check
- **Grok**: Practical validation

**Result**: Quick check + exhaustive exploration + validation = comprehensive understanding

### Execution Pattern

Same as Strategy 1/2, launch 3 models in parallel.

### Expected Output

**MiniMax M2**:
```
Initial hypothesis: [simplest explanation]
```

**Gemini 2.5 Flash**:
```
Explored hypotheses:
1. [Theory A]
2. [Theory B]
3. [Theory C]
Most likely: [detailed analysis]
```

**Grok Code Fast**:
```
Validation approach for each hypothesis: [tests]
```

### Consolidation

1. **Start with MiniMax M2's hypothesis** (usually simplest)
2. **If that fails**, use Gemini's alternative theories
3. **Validate** with Grok's test strategy

### Use Case Example

**Intermittent Race Condition**:

**Symptoms**: Test fails randomly, can't reproduce consistently

**MiniMax M2**: "Check for global state mutation"
**Gemini**: "Could be: 1) Race condition, 2) Test order dependency, 3) External API timing, 4) Cache state, 5) Goroutine leak"
**Grok**: "Run tests 100x in parallel, add synchronization points, log timing data"

**Result**: Used Gemini's hypothesis #1, validated with Grok's approach, found race condition

---

## Strategy 4: Budget-Conscious

**Models**: `google/gemini-2.5-flash` + `x-ai/grok-code-fast-1`
**Time**: ~6 minutes total
**Success Rate**: 85%+
**Cost**: $ (low)
**Use**: Cost-sensitive projects, exploratory work

### When to Use

✅ **Perfect for**:
- Cost-sensitive projects
- Non-critical investigations
- Exploratory analysis
- Learning and experimentation
- When MiniMax M2 budget is constrained

❌ **Not ideal for**:
- Production critical bugs (use Strategy 1 or 2)
- When you need highest accuracy
- Time-critical issues

### Why This Works

**Gemini 2.5 Flash**: Low cost, thorough analysis
**Grok Code Fast**: Moderate cost, practical validation

**Together**: Good coverage at low total cost

### Execution Pattern

Launch 2 models in parallel (like Strategy 1, but Gemini instead of MiniMax M2).

### Expected Output

**Gemini**:
```
Thorough analysis with multiple angles: [detailed]
```

**Grok**:
```
Practical validation: [testing approach]
```

### Trade-offs

**Pros**:
- Low cost ($ + $$ = ~half of Strategy 1)
- Still dual perspectives
- Good for exploration

**Cons**:
- Lower success rate (85% vs 95%)
- May miss simplest solution
- Longer execution time (6 min vs 4 min)

### Use Case Example

**Experimental Feature Investigation**:

**Question**: How to implement lambda syntax for Dingo?

**Gemini**: Exhaustive analysis of options: arrow functions, Ruby blocks, Python lambdas, Go function literals
**Grok**: Test each approach with examples, measure code generation complexity

**Result**: Good exploration at low cost, decided on arrow function syntax

---

## Comparison Matrix

| Strategy | Models | Time | Cost | Success | Best For |
|----------|--------|------|------|---------|----------|
| 1. Fast | M2 + Grok | 4 min | $$ | 95% | Bug investigations (90% of cases) |
| 2. Comprehensive | M2 + GPT + Grok | 5 min | $$$ | 99% | Critical bugs, architecture |
| 3. Deep Exploration | M2 + Gemini + Grok | 6 min | $$ | 90% | Ambiguous problems |
| 4. Budget | Gemini + Grok | 6 min | $ | 85% | Cost-sensitive, exploratory |

---

## Choosing the Right Strategy

### Quick Decision Tree

```
START: Need external model consultation
    ↓
[Is this a production critical issue?]
    ↓ YES
    [Strategy 2: Comprehensive]
    (M2 + GPT-5.1 + Grok)
    ↓ NO
    ↓
[Is the root cause unclear/ambiguous?]
    ↓ YES
    [Strategy 3: Deep Exploration]
    (M2 + Gemini + Grok)
    ↓ NO
    ↓
[Is cost a major constraint?]
    ↓ YES
    [Strategy 4: Budget]
    (Gemini + Grok)
    ↓ NO
    ↓
[Default: Strategy 1: Fast Diagnosis]
(M2 + Grok)
```

### Rules of Thumb

**Use Strategy 1 (Fast) when**:
- Bug is fairly straightforward
- You need quick turnaround
- This is an everyday task
- 90% of your use cases

**Use Strategy 2 (Comprehensive) when**:
- Stakes are high (production impact)
- Long-term architectural decisions
- Need absolute certainty
- Can justify higher cost

**Use Strategy 3 (Deep Exploration) when**:
- Initial investigation inconclusive
- Multiple potential causes
- Complex system interactions
- Need exhaustive analysis

**Use Strategy 4 (Budget) when**:
- Cost is primary concern
- Non-critical exploration
- Learning new codebase
- Experimental features

---

## Common Patterns

### Sequential Escalation

Start cheap, escalate if needed:

1. **Try Strategy 1** (Fast): M2 + Grok
2. **If unsuccessful** → Strategy 3 (Deep): Add Gemini
3. **If still unclear** → Strategy 2 (Comprehensive): Add GPT-5.1

**Total cost**: Only pay for what you need

### Parallel Comparison

Run multiple strategies simultaneously:

1. **Launch Strategy 1 AND 3** in parallel
2. **Compare results** after 6 minutes
3. **Choose best approach**

**Trade-off**: Higher cost, but maximum confidence

### Iterative Refinement

Use different strategies at different phases:

1. **Phase 1**: Strategy 4 (Budget) for exploration
2. **Phase 2**: Strategy 1 (Fast) for implementation
3. **Phase 3**: Strategy 2 (Comprehensive) for validation

**Benefit**: Appropriate investment at each phase

---

## Success Metrics

Track these metrics to validate strategy effectiveness:

### Accuracy Metrics

- Did the models identify correct root cause?
- Was the proposed solution correct?
- How many iterations needed?

### Efficiency Metrics

- Time to diagnosis
- Time to implementation
- Cost per successful investigation

### Quality Metrics

- Solution simplicity
- Code quality after fix
- Regression rate

### ROI Metrics

- Cost vs value delivered
- Time saved vs manual investigation
- Defect prevention rate

---

## Conclusion

**Most common**: Strategy 1 (Fast) - 90% of use cases
**Highest success**: Strategy 2 (Comprehensive) - 99%+ accuracy
**Best value**: Strategy 1 (Fast) - 95% success at moderate cost
**Lowest cost**: Strategy 4 (Budget) - 85% success at low cost

**Remember**:
- Start with Strategy 1 for most tasks
- Escalate to Strategy 2 for critical issues
- Use Strategy 3 for ambiguous problems
- Use Strategy 4 when cost is primary concern

**All strategies use**:
- 10-minute timeout (critical!)
- Parallel execution (6-8x speedup)
- File-based communication (context economy)
- Brief summaries (MAX 3 lines)
