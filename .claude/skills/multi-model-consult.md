# Multi-Model Consultation Skill

You are executing the **Multi-Model Consultation** pattern. This skill helps you consult multiple external LLMs in parallel to get diverse perspectives on architectural decisions, design choices, or complex analysis.

## Your Task

The user wants perspectives from multiple external models. Follow these steps EXACTLY:

### Step 1: Create Session Folder

```bash
SESSION=$(date +%Y%m%d-%H%M%S)
mkdir -p ai-docs/sessions/$SESSION/{input,output}
```

### Step 2: Write Investigation Prompt

Extract the user's question/investigation topic and write a comprehensive prompt to:
`ai-docs/sessions/$SESSION/input/investigation-prompt.md`

The prompt should:
- Clearly state the question/problem
- Provide necessary context about Dingo project
- Ask for specific analysis or recommendations
- Be self-contained (model won't have conversation history)

### Step 3: Identify Models to Consult

**EVIDENCE-BASED RECOMMENDATIONS** (validated via Session 20251118-223538):

#### ü•á Tier 1: Primary Recommendations (Use First)

**Default Fast Diagnosis** (90% of use cases):
- `minimax/minimax-m2` - **BEST PERFORMER** (Score: 91/100, 3 min, pinpoint accuracy)
- `x-ai/grok-code-fast-1` - **DEBUGGING EXPERT** (Score: 83/100, 4 min, excellent traces)

**Default Comprehensive Analysis**:
- `minimax/minimax-m2` - Fast + accurate
- `openai/gpt-5.1-codex` - Architectural vision (Score: 80/100, 5 min)
- `x-ai/grok-code-fast-1` - Validation + testing

#### ü•à Tier 2: Specialized Use Cases

- `google/gemini-2.5-flash` - Ambiguous problems, exhaustive analysis (Score: 73/100, 6 min, LOW COST)
- `z-ai/glm-4.6` - Algorithm enhancements, debug infrastructure (Score: 70/100, 7 min)

#### ‚ö†Ô∏è Use With Caution

- `openrouter/sherlock-think-alpha` - Protocol compliance only (Score: 65/100, 5 min, HIGH COST)

#### ‚ùå AVOID (Known Issues)

- `qwen/qwen3-coder-30b-a3b-instruct` - **UNRELIABLE** (Timeout after 8+ min, 0% success rate)

#### Other Available Models (Not Yet Validated)

- `openai/gpt-5` - Most advanced reasoning (not tested yet)
- `qwen/qwen3-vl-235b-a22b-instruct` - Multimodal (not tested yet)
- `openrouter/polaris-alpha` - FREE experimental (not tested yet)

**Validation Date**: 2025-11-18 | **Re-test**: Every 3-6 months

---

### Step 3.5: Choose Consultation Strategy

Based on task type and priority, select one of these proven strategies:

#### Strategy 1: Fast Parallel Diagnosis (DEFAULT - 90% of use cases)
```
Models: minimax/minimax-m2 + x-ai/grok-code-fast-1
Time: ~4 minutes total
Use for: Bug investigations, quick diagnosis
Benefits: Fast fix + validation
```

#### Strategy 2: Comprehensive Analysis (Critical bugs)
```
Models: minimax/minimax-m2 + openai/gpt-5.1-codex + x-ai/grok-code-fast-1
Time: ~5 minutes total
Use for: Critical bugs, architectural decisions
Benefits: Quick fix + long-term plan + validation
```

#### Strategy 3: Deep Exploration (Ambiguous problems)
```
Models: minimax/minimax-m2 + google/gemini-2.5-flash + x-ai/grok-code-fast-1
Time: ~6 minutes total
Use for: Ambiguous, multi-faceted problems
Benefits: Quick fix + exhaustive analysis + validation
```

#### Strategy 4: Budget-Conscious (Cost-sensitive)
```
Models: google/gemini-2.5-flash + x-ai/grok-code-fast-1
Time: ~6 minutes total
Use for: When minimizing cost is priority
Benefits: Low-cost exploration + good validation
```

**Decision Tree**:
```
[What's the task?]
  ‚Üì
[Bug Investigation?] ‚Üí Strategy 1 (MiniMax + Grok)
[Architectural Decision?] ‚Üí Strategy 2 (MiniMax + GPT-5.1 + Grok)
[Ambiguous Problem?] ‚Üí Strategy 3 (MiniMax + Gemini + Grok)
[Cost-Sensitive?] ‚Üí Strategy 4 (Gemini + Grok)
```

---

### Step 4: Select Appropriate Agent Type

Based on the domain:
- **Go project questions** (parser, AST, transpiler) ‚Üí `golang-architect`
- **Astro/landing page questions** ‚Üí `astro-developer`
- **General code review** ‚Üí `code-reviewer`
- **Multi-language** ‚Üí `general-purpose` (last resort)

### Step 5: Launch Agents in Parallel

**CRITICAL**: Launch ALL agents in a **SINGLE MESSAGE** with multiple Task tool calls.

For each model, create a Task call like this:

```
Task tool ‚Üí [agent-type]:

You are operating in PROXY MODE to [task description] using [model-name].

INPUT FILES:
- Investigation prompt: ai-docs/sessions/$SESSION/input/investigation-prompt.md

YOUR TASK (PROXY MODE):
1. Read the investigation prompt from input file
2. Use claudish to invoke [model-name] (model ID: [model-id])
3. Provide the model with the investigation prompt
4. Write complete response to output file

**CRITICAL - Timeout Configuration**:
When executing claudish via Bash tool, ALWAYS use:
```
Bash(
    command='cat ai-docs/sessions/$SESSION/input/investigation-prompt.md | claudish --model [model-id] > ai-docs/sessions/$SESSION/output/[model-name]-analysis.md 2>&1',
    timeout=600000,  # 10 minutes (REQUIRED - default 2 min will timeout!)
    description='External consultation via [model-name]'
)
```

**Why 10-minute timeout?**: External models take 5-10 minutes. Default 2-minute timeout causes failures.

OUTPUT FILES (write full details here):
- ai-docs/sessions/$SESSION/output/[model-name]-analysis.md - Complete analysis

RETURN MESSAGE (keep this brief - MAX 3 lines):
Return ONLY this format:
[Model-name] analysis complete
Root cause: [one-line summary]
Full analysis: ai-docs/sessions/$SESSION/output/[model-name]-analysis.md

DO NOT return the full analysis in your response - it causes context bloat.
```

**Example** (3 models in parallel):
- Launch 3 Task calls in ONE message
- Each Task uses same agent type (e.g., golang-architect)
- Each Task invokes different model
- Each Task saves to different output file

### Step 6: Aggregate Results

After receiving all summaries:
1. Present brief overview to user (which models were consulted)
2. Show 1-sentence key finding from each model
3. Provide file paths for detailed analyses
4. Ask if user wants a consolidation analysis

### Step 7: Optional Consolidation

If user wants synthesis:
- Launch ONE final agent (same type)
- Agent reads ALL analysis files
- Agent synthesizes consensus + disagreements
- Agent writes: `ai-docs/sessions/$SESSION/output/CONSOLIDATED.md`
- Agent returns brief summary

## Key Rules

1. ‚úÖ **Always use specialized agents** (golang-architect for Go, etc.)
2. ‚úÖ **Always launch in parallel** (single message, multiple Task calls)
3. ‚úÖ **Each agent = one model** (1:1 mapping)
4. ‚úÖ **Communication via files** (full analysis ‚Üí files, brief summary ‚Üí response)
5. ‚úÖ **Agent uses Bash** to invoke claudish (not Task tool)
6. ‚ùå **Never use general-purpose** unless no specialized agent exists

## Example Execution

```
User: "Should we use regex preprocessor or migrate to tree-sitter?"

You (main chat):
1. Create session: ai-docs/sessions/20251118-160000/
2. Write investigation-prompt.md with detailed question
3. Launch 3 golang-architect agents IN PARALLEL (one message):
   - Task 1: gpt-5.1-codex
   - Task 2: gemini-2.5-flash
   - Task 3: grok-code-fast-1
4. Receive 3 summaries
5. Present to user:
   "Consulted 3 models:
    - GPT-5.1-Codex: Recommends regex for simplicity, tree-sitter for future
    - Gemini-2.5-Flash: Suggests hybrid approach
    - Grok: Advocates staying with regex

    Details: ai-docs/sessions/20251118-160000/output/
    Want me to synthesize these perspectives?"
```

## Success Metrics

- **Speed**: 2-3 minutes (parallel) vs 5-10 minutes (sequential)
- **Context**: 50-100 lines (files) vs 500-1000 lines (inline)
- **Quality**: Diverse perspectives + domain expertise

## What to Return to User

After execution completes:
1. Brief summary of what models were consulted
2. One-line key insight from each model
3. Session folder path
4. Ask if consolidation needed

**Total output**: < 20 lines in main chat
**Detailed analyses**: In session files

---

**Remember**: You are the orchestrator. Delegate the actual model invocations to specialized agents. Keep main chat lean!

---

## Performance Benchmarks (Evidence-Based)

**Validation Task**: LSP Source Mapping Bug (Session 20251118-223538)
**Problem**: Diagnostic underlining wrong code segment
**8 Models Tested** (7 external + 1 internal Sonnet 4.5)

### Results Summary

| Model | Time | Accuracy | Solution | Value |
|-------|------|----------|----------|-------|
| MiniMax M2 | 3 min | ‚úÖ Exact | Simple fix | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Sonnet 4.5 (Internal) | 4 min | ‚úÖ Exact | Complete plan | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Grok Code Fast | 4 min | ‚úÖ Correct | Good validation | ‚≠ê‚≠ê‚≠ê‚≠ê |
| GPT-5.1 Codex | 5 min | ‚ö†Ô∏è Partial | Complex design | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Gemini 2.5 Flash | 6 min | ‚ö†Ô∏è Missed | Overanalyzed | ‚≠ê‚≠ê‚≠ê |
| GLM-4.6 | 7 min | ‚ùå Wrong focus | Overengineered | ‚≠ê‚≠ê |
| Sherlock Think | 5 min | ‚ùå Secondary | Wrong cause | ‚≠ê‚≠ê |
| Qwen3 Coder | 8+ min | ‚ùå Failed | Timeout | ‚ö†Ô∏è |

**Key Finding**: **Faster models delivered better results**. Speed correlates with focus on simplicity.

### Cost-Effectiveness Rankings

1. **MiniMax M2** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - Best value, fastest accurate diagnosis
2. **Sonnet 4.5** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - Best if using internal (free)
3. **Grok Code Fast** (‚≠ê‚≠ê‚≠ê‚≠ê) - Excellent debugging + validation
4. **Gemini 2.5 Flash** (‚≠ê‚≠ê‚≠ê) - Low cost, good for exploration
5. **GPT-5.1 Codex** (‚≠ê‚≠ê‚≠ê) - High value for architectural work

### What Made Top Models Successful

‚úÖ **Focus on simplicity** - Identified simplest root cause first
‚úÖ **Code-level precision** - Referenced specific files/line numbers
‚úÖ **Practical solutions** - Proposed implementable fixes
‚úÖ **Fast execution** - Completed in 3-5 minutes

### What Held Lower-Ranked Models Back

‚ùå **Overengineering** - Added unnecessary complexity
‚ùå **Going too deep** - Explored hypothetical scenarios, missed simple bug
‚ùå **Secondary issues** - Focused on symptoms, not root cause
‚ùå **Reliability** - Timeouts and failures

### Recommended Parallel Combinations

**For Maximum Success** (based on empirical data):
- **Bug Investigation**: MiniMax M2 + Grok Code Fast (95%+ success)
- **Architectural**: MiniMax M2 + GPT-5.1 + Grok (99%+ success)
- **Budget**: Gemini 2.5 + Grok (85%+ success, low cost)

**Full Report**: `ai-docs/sessions/20251118-223538/01-planning/comprehensive-model-comparison.md`

---

**Last Updated**: 2025-11-18 | **Validation Session**: 20251118-223538 | **Next Review**: 2025-05
